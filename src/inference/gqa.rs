//! Grouped-Query Attention (GQA) Optimization
//!
//! v0.8: Dedicated optimized path for GQA models (Llama 2 70B, Llama 3, Mixtral, etc.)
//! where n_head > n_head_kv. Instead of naively repeating KV heads in the attention loop,
//! we batch query heads that share the same KV head and compute their attention together.
//!
//! Key optimizations:
//! - Grouped score computation: compute KV dot products once per KV group
//! - SIMD-friendly memory layout for grouped heads
//! - Fused softmax + weighted sum per group
//! - Reduced memory bandwidth by avoiding KV head repetition

use crate::inference::kv_cache::LayerKvCache;
use crate::metal::compute::{matvec_f32_simd, softmax_f32_fast};

/// GQA-optimized attention: groups query heads sharing the same KV head
/// and computes their attention scores together, loading each KV entry once per group.
///
/// For a model with n_head=32 and n_head_kv=8, this means:
/// - 8 groups of 4 query heads each
/// - Each KV head's keys/values are loaded once per group instead of 4 times
/// - ~4x reduction in KV cache memory reads
pub fn gqa_attention(
    x: &[f32],
    wq: &[f32],
    wk: &[f32],
    wv: &[f32],
    wo: &[f32],
    n_head: usize,
    n_head_kv: usize,
    head_dim: usize,
    position: usize,
    kv_cache: &mut LayerKvCache,
) -> Vec<f32> {
    let n_embd = x.len();
    let q_dim = n_head * head_dim;
    let kv_dim = n_head_kv * head_dim;
    let group_size = n_head / n_head_kv;

    // Project Q, K, V
    let mut q = matvec_f32_simd(wq, x, q_dim, n_embd);
    let mut k = matvec_f32_simd(wk, x, kv_dim, n_embd);
    let v = matvec_f32_simd(wv, x, kv_dim, n_embd);

    // Apply RoPE
    apply_rope_inplace(&mut q, head_dim, n_head, position);
    apply_rope_inplace(&mut k, head_dim, n_head_kv, position);

    // Append to cache
    kv_cache.append(k, v);

    let seq_len = kv_cache.seq_len();
    let scale = 1.0 / (head_dim as f32).sqrt();

    let mut attn_output = vec![0.0f32; n_embd];

    // Process by KV head groups â€” each KV head serves `group_size` query heads
    for kv_h in 0..n_head_kv {
        // Pre-fetch all key/value vectors for this KV head (loaded once for all Q heads in group)
        // This is the key optimization: avoid re-reading the same KV entries for each Q head
        let cached_keys: Vec<&[f32]> = (0..seq_len)
            .map(|pos| kv_cache.key_at(pos, kv_h))
            .collect();
        let cached_values: Vec<&[f32]> = (0..seq_len)
            .map(|pos| kv_cache.value_at(pos, kv_h))
            .collect();

        // Process all query heads in this group
        for g in 0..group_size {
            let h = kv_h * group_size + g;
            let q_offset = h * head_dim;
            let q_head = &q[q_offset..q_offset + head_dim];

            // Compute attention scores using pre-fetched keys
            let mut scores = Vec::with_capacity(seq_len);
            for pos in 0..seq_len {
                let k_head = cached_keys[pos];
                let dot = dot_product_simd(q_head, k_head, head_dim);
                scores.push(dot * scale);
            }

            softmax_f32_fast(&mut scores);

            // Weighted sum using pre-fetched values
            let out_offset = h * head_dim;
            for d in 0..head_dim {
                let mut weighted = 0.0f32;
                for pos in 0..seq_len {
                    weighted += scores[pos] * cached_values[pos][d];
                }
                if out_offset + d < attn_output.len() {
                    attn_output[out_offset + d] = weighted;
                }
            }
        }
    }

    matvec_f32_simd(wo, &attn_output, n_embd, n_embd)
}

/// GQA attention with sliding window support
pub fn gqa_sliding_window_attention(
    x: &[f32],
    wq: &[f32],
    wk: &[f32],
    wv: &[f32],
    wo: &[f32],
    n_head: usize,
    n_head_kv: usize,
    head_dim: usize,
    position: usize,
    kv_cache: &mut LayerKvCache,
    window_size: usize,
    sink_tokens: usize,
) -> Vec<f32> {
    let n_embd = x.len();
    let q_dim = n_head * head_dim;
    let kv_dim = n_head_kv * head_dim;
    let group_size = n_head / n_head_kv;

    let mut q = matvec_f32_simd(wq, x, q_dim, n_embd);
    let mut k = matvec_f32_simd(wk, x, kv_dim, n_embd);
    let v = matvec_f32_simd(wv, x, kv_dim, n_embd);

    apply_rope_inplace(&mut q, head_dim, n_head, position);
    apply_rope_inplace(&mut k, head_dim, n_head_kv, position);

    kv_cache.append(k, v);

    let seq_len = kv_cache.seq_len();
    let scale = 1.0 / (head_dim as f32).sqrt();

    // Compute visible positions
    let positions: Vec<usize> = if window_size > 0 && seq_len > window_size + sink_tokens {
        let window_start = seq_len.saturating_sub(window_size);
        let sink_end = sink_tokens.min(window_start);
        (0..sink_end).chain(window_start..seq_len).collect()
    } else {
        (0..seq_len).collect()
    };

    let mut attn_output = vec![0.0f32; n_embd];

    for kv_h in 0..n_head_kv {
        // Pre-fetch KV for visible positions only
        let cached_keys: Vec<&[f32]> = positions.iter()
            .map(|&pos| kv_cache.key_at(pos, kv_h))
            .collect();
        let cached_values: Vec<&[f32]> = positions.iter()
            .map(|&pos| kv_cache.value_at(pos, kv_h))
            .collect();

        for g in 0..group_size {
            let h = kv_h * group_size + g;
            let q_offset = h * head_dim;
            let q_head = &q[q_offset..q_offset + head_dim];

            let mut scores: Vec<f32> = cached_keys.iter()
                .map(|k_head| dot_product_simd(q_head, k_head, head_dim) * scale)
                .collect();

            softmax_f32_fast(&mut scores);

            let out_offset = h * head_dim;
            for d in 0..head_dim {
                let mut weighted = 0.0f32;
                for (i, v_head) in cached_values.iter().enumerate() {
                    weighted += scores[i] * v_head[d];
                }
                if out_offset + d < attn_output.len() {
                    attn_output[out_offset + d] = weighted;
                }
            }
        }
    }

    matvec_f32_simd(wo, &attn_output, n_embd, n_embd)
}

/// SIMD-friendly dot product with 4-wide accumulation
#[inline]
fn dot_product_simd(a: &[f32], b: &[f32], len: usize) -> f32 {
    let chunks = len / 4;
    let mut acc0 = 0.0f32;
    let mut acc1 = 0.0f32;
    let mut acc2 = 0.0f32;
    let mut acc3 = 0.0f32;

    for c in 0..chunks {
        let i = c * 4;
        acc0 += a[i] * b[i];
        acc1 += a[i + 1] * b[i + 1];
        acc2 += a[i + 2] * b[i + 2];
        acc3 += a[i + 3] * b[i + 3];
    }

    let mut result = (acc0 + acc1) + (acc2 + acc3);

    // Remainder
    for i in (chunks * 4)..len {
        result += a[i] * b[i];
    }

    result
}

/// Apply RoPE in-place
fn apply_rope_inplace(x: &mut [f32], head_dim: usize, n_heads: usize, position: usize) {
    let theta_base = 10000.0f32;
    for h in 0..n_heads {
        let base = h * head_dim;
        for i in (0..head_dim).step_by(2) {
            let freq = 1.0 / theta_base.powf(i as f32 / head_dim as f32);
            let angle = position as f32 * freq;
            let (sin_val, cos_val) = angle.sin_cos();
            let idx0 = base + i;
            let idx1 = base + i + 1;
            if idx1 < x.len() {
                let x0 = x[idx0];
                let x1 = x[idx1];
                x[idx0] = x0 * cos_val - x1 * sin_val;
                x[idx1] = x0 * sin_val + x1 * cos_val;
            }
        }
    }
}

/// Determine optimal attention strategy based on model configuration
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum AttentionStrategy {
    /// Standard MHA (n_head == n_head_kv)
    MultiHead,
    /// Grouped-Query Attention (n_head > n_head_kv > 1)
    GroupedQuery { group_size: usize },
    /// Multi-Query Attention (n_head_kv == 1)
    MultiQuery,
}

impl AttentionStrategy {
    pub fn detect(n_head: usize, n_head_kv: usize) -> Self {
        if n_head_kv == 1 {
            AttentionStrategy::MultiQuery
        } else if n_head == n_head_kv {
            AttentionStrategy::MultiHead
        } else {
            AttentionStrategy::GroupedQuery {
                group_size: n_head / n_head_kv,
            }
        }
    }

    pub fn description(&self) -> &'static str {
        match self {
            AttentionStrategy::MultiHead => "MHA (Multi-Head Attention)",
            AttentionStrategy::GroupedQuery { group_size } => "GQA (Grouped-Query Attention)",
            AttentionStrategy::MultiQuery => "MQA (Multi-Query Attention)",
        }
    }

    /// KV memory savings ratio compared to full MHA
    pub fn kv_savings_ratio(&self, n_head: usize) -> f32 {
        match self {
            AttentionStrategy::MultiHead => 1.0,
            AttentionStrategy::GroupedQuery { group_size } => 1.0 / *group_size as f32,
            AttentionStrategy::MultiQuery => 1.0 / n_head as f32,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::inference::kv_cache::LayerKvCache;

    #[test]
    fn test_gqa_attention_basic() {
        let n_embd = 16;
        let n_head = 4;
        let n_head_kv = 2; // GQA: 2 groups of 2
        let head_dim = 4;
        let mut kv = LayerKvCache::new(n_head_kv, head_dim);

        let x = vec![1.0f32; n_embd];
        let wq = vec![0.01f32; n_head * head_dim * n_embd];
        let wk = vec![0.01f32; n_head_kv * head_dim * n_embd];
        let wv = vec![0.01f32; n_head_kv * head_dim * n_embd];
        let wo = vec![0.01f32; n_embd * n_embd];

        let result = gqa_attention(&x, &wq, &wk, &wv, &wo, n_head, n_head_kv, head_dim, 0, &mut kv);
        assert_eq!(result.len(), n_embd);
        assert_eq!(kv.seq_len(), 1);
    }

    #[test]
    fn test_gqa_multi_token() {
        let n_embd = 8;
        let n_head = 4;
        let n_head_kv = 2;
        let head_dim = 2;
        let mut kv = LayerKvCache::new(n_head_kv, head_dim);

        let x = vec![0.5f32; n_embd];
        let w = vec![0.01f32; n_embd * n_embd];
        let wk = vec![0.01f32; n_head_kv * head_dim * n_embd];
        let wv = vec![0.01f32; n_head_kv * head_dim * n_embd];

        for pos in 0..5 {
            let _ = gqa_attention(&x, &w, &wk, &wv, &w, n_head, n_head_kv, head_dim, pos, &mut kv);
        }
        assert_eq!(kv.seq_len(), 5);
    }

    #[test]
    fn test_gqa_sliding_window() {
        let n_embd = 8;
        let n_head = 4;
        let n_head_kv = 2;
        let head_dim = 2;
        let mut kv = LayerKvCache::new(n_head_kv, head_dim);

        let x = vec![0.5f32; n_embd];
        let w = vec![0.01f32; n_embd * n_embd];
        let wk = vec![0.01f32; n_head_kv * head_dim * n_embd];
        let wv = vec![0.01f32; n_head_kv * head_dim * n_embd];

        for pos in 0..8 {
            let _ = gqa_sliding_window_attention(
                &x, &w, &wk, &wv, &w, n_head, n_head_kv, head_dim, pos, &mut kv, 4, 0,
            );
        }
        assert_eq!(kv.seq_len(), 8);
    }

    #[test]
    fn test_dot_product_simd() {
        let a = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let b = vec![2.0, 3.0, 4.0, 5.0, 6.0];
        let result = dot_product_simd(&a, &b, 5);
        let expected: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
        assert!((result - expected).abs() < 1e-5);
    }

    #[test]
    fn test_attention_strategy_detection() {
        assert_eq!(AttentionStrategy::detect(32, 32), AttentionStrategy::MultiHead);
        assert_eq!(AttentionStrategy::detect(32, 8), AttentionStrategy::GroupedQuery { group_size: 4 });
        assert_eq!(AttentionStrategy::detect(32, 1), AttentionStrategy::MultiQuery);
    }

    #[test]
    fn test_kv_savings_ratio() {
        let mha = AttentionStrategy::MultiHead;
        assert_eq!(mha.kv_savings_ratio(32), 1.0);

        let gqa = AttentionStrategy::GroupedQuery { group_size: 4 };
        assert_eq!(gqa.kv_savings_ratio(32), 0.25);

        let mqa = AttentionStrategy::MultiQuery;
        assert!((mqa.kv_savings_ratio(32) - 1.0 / 32.0).abs() < 1e-6);
    }
}
