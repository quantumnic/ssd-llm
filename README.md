# ssd-llm ğŸš€

**Run 70B+ LLMs on Apple Silicon by using SSD as extended memory.**

Intelligent layer streaming and caching for Mac â€” no need for 128GB RAM.

## The Problem

Large language models like LLaMA 70B require ~40GB+ RAM even with 4-bit quantization. Most MacBooks have 16â€“36GB unified memory. You either:
- Can't run the model at all
- Use llama.cpp's mmap, which thrashes your SSD with no intelligence
- Accept terrible performance from OS swap pressure

## The Solution

**ssd-llm** treats your fast Apple SSD as an intelligent extension of RAM:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SSD (2TB)  â”‚â”€â”€â”€â”€â–¶â”‚ Smart Cache  â”‚â”€â”€â”€â”€â–¶â”‚ Metal GPU â”‚
â”‚  Model File â”‚     â”‚ (Layer Pool) â”‚     â”‚ Inference  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–²                    â”‚
                     â”‚    Prefetch        â”‚ Compute
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Instead of loading the entire model, **ssd-llm** streams transformer layers on-demand from SSD to unified memory, computes them, and frees the memory. Predictive prefetching ensures the next layer is already loading while the current one is being computed.

## Key Features

- **ğŸ§± Layer-Level Streaming** â€” Only 1-2 transformer layers in RAM at once
- **ğŸ”® Predictive Prefetching** â€” Next layer loads asynchronously via `madvise(MADV_WILLNEED)` while GPU computes
- **ğŸ“¦ Smart LRU Cache** â€” Frequently used layers (embeddings, early attention) stay pinned in RAM
- **ğŸ—ºï¸ mmap + madvise** â€” OS-level memory-mapped files with intelligent page hints
- **âš¡ Metal Compute** â€” SIMD-optimized matmul, softmax, RoPE, RMSNorm with Metal shader foundation
- **ğŸ“„ GGUF Support** â€” Compatible with llama.cpp quantization formats (Q4_0, Q8_0, F16, F32)
- **ğŸ”Œ Ollama-compatible API** â€” Drop-in replacement server with OpenAI-compatible endpoint

## Quick Start

```bash
# Build
cargo build --release

# Show model info
ssd-llm info model.gguf

# Run inference with 8GB memory budget
ssd-llm run model.gguf --memory-budget 8G --prompt "Explain quantum computing"

# Benchmark SSD streaming performance
ssd-llm bench model.gguf --memory-budget 8G

# Start Ollama-compatible API server
ssd-llm serve model.gguf --memory-budget 8G --port 11434
```

## API Server

The `serve` command starts an Ollama-compatible HTTP server:

```bash
ssd-llm serve model.gguf --memory-budget 8G
```

### Endpoints

| Endpoint | Method | Description |
|---|---|---|
| `/api/generate` | POST | Text generation (Ollama format) |
| `/api/chat` | POST | Chat completion (Ollama format) |
| `/api/tags` | GET | List loaded models |
| `/api/version` | GET | Server version |
| `/v1/chat/completions` | POST | OpenAI-compatible chat |

### Usage with curl

```bash
# Ollama-style generation
curl -X POST http://localhost:11434/api/generate \
  -d '{"prompt": "What is Rust?", "num_predict": 128}'

# OpenAI-compatible chat
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello!"}], "max_tokens": 128}'
```

## How It Works

### Layer Streaming Architecture

Traditional LLM inference loads the entire model into RAM. **ssd-llm** takes a different approach:

1. **GGUF Parser** reads model metadata and tensor offsets without loading data
2. **mmap Loader** memory-maps the model file â€” the OS handles page faults
3. **Predictive Prefetcher** issues `madvise(MADV_WILLNEED)` for the next layer while the current one computes
4. **LRU Cache** keeps hot layers (embeddings, output weights) pinned in memory
5. **Eviction** calls `madvise(MADV_DONTNEED)` on completed layers to free page cache

### Why Apple Silicon?

Apple's Unified Memory Architecture is uniquely suited for this:

| Feature | Apple Silicon | Traditional PC |
|---|---|---|
| Memory | Unified (CPU+GPU shared) | Separate RAM + VRAM |
| SSD Speed | 5-7 GB/s (M3/M4 Pro) | 3-5 GB/s (NVMe) |
| Memory Bandwidth | 200-800 GB/s | 50-100 GB/s (DDR5) |
| GPU Access | Direct to unified memory | PCIe copy required |

The fast SSD + unified memory means layer streaming has very low overhead on Mac.

## Benchmarks

> v0.3 â€” KV cache, Metal shaders compiled, SwiGLU FFN, quantized GPU kernels

| Model | Quant | Size | Memory Budget | Layer Load | Est. tok/s |
|---|---|---|---|---|---|
| LLaMA 7B | Q4_0 | 3.5 GB | 4 GB | ~2ms/layer | TBD |
| LLaMA 13B | Q4_0 | 7 GB | 8 GB | ~4ms/layer | TBD |
| LLaMA 70B | Q4_0 | 35 GB | 8 GB | ~8ms/layer | TBD |

Run `ssd-llm bench` on your machine to get actual numbers.

## Comparison

| Feature | ssd-llm | llama.cpp | Ollama |
|---|---|---|---|
| SSD Streaming | âœ… Intelligent | âš ï¸ Naive mmap | âŒ Full RAM |
| Predictive Prefetch | âœ… madvise hints | âŒ | âŒ |
| Memory Budget | âœ… Configurable | âŒ | âŒ |
| Layer-level Cache | âœ… LRU + pinning | âŒ | âŒ |
| Metal GPU | âœ… Shaders + SIMD | âœ… | âœ… (via llama.cpp) |
| GGUF Support | âœ… | âœ… | âœ… |
| Quantization | Q4_0, Q8_0, F16 | All | All |
| API Server | âœ… Ollama + OpenAI | âœ… | âœ… |

## Architecture

```
src/
  main.rs              â€” CLI + entry point
  model/
    gguf.rs            â€” GGUF v2/v3 parser
    loader.rs          â€” mmap-based lazy loader
    cache.rs           â€” LRU layer cache with memory budget
  inference/
    transformer.rs     â€” Layer-by-layer forward pass
    attention.rs       â€” Multi-Head Attention with KV cache (GQA support)
    kv_cache.rs        â€” Key-Value cache for autoregressive generation
    feed_forward.rs    â€” SwiGLU FFN
    sampler.rs         â€” Temperature, Top-K, Top-P sampling (xorshift64)
    tokenizer.rs       â€” Basic tokenizer from GGUF vocab
  metal/
    compute.rs         â€” Metal compute + SIMD-optimized ops
    shaders/           â€” .metal compute shaders (matmul, rmsnorm, rope, softmax)
  ssd/
    streamer.rs        â€” SSD â†’ RAM streaming engine
    prefetch.rs        â€” Predictive prefetcher
    mmap_pool.rs       â€” mmap pool with madvise management
  api/
    server.rs          â€” Ollama-compatible HTTP API server
    openai.rs          â€” OpenAI-compatible types + ChatML formatting
  benchmark.rs         â€” Performance measurement
```

## Prior Art & Research

This project builds on insights from:

- **llama.cpp** â€” Uses mmap but with no intelligent page management
- **FlexGen** â€” SSD offloading for throughput-oriented inference
- **PowerInfer** â€” Sparsity-based selective loading
- **LLM in a Flash** (Apple Research) â€” Flash memory optimization for LLM inference
- **FlexInfer** â€” Flexible offloading with computation-I/O overlap

## Roadmap

- [x] v0.1 â€” GGUF parser, mmap loader, LRU cache, prefetcher, CPU inference
- [x] v0.2 â€” Metal compute foundation, SIMD ops, Ollama + OpenAI API server
- [x] v0.3 â€” KV cache, Metal shader compilation, SwiGLU FFN, quantized GPU kernels (Q4_0/Q8_0)
- [ ] v0.4 â€” Full Metal GPU dispatch via metal-rs, BPE tokenizer, streaming responses
- [ ] v0.5 â€” Speculative decoding with draft model
- [ ] v1.0 â€” Production-ready, benchmarked against llama.cpp

## Requirements

- macOS 13+ (Apple Silicon recommended)
- Rust 1.75+
- GGUF model file (from [HuggingFace](https://huggingface.co/models?library=gguf))

## License

MIT
