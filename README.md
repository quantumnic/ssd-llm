# ssd-llm ğŸš€

**Run 70B+ LLMs on Apple Silicon by using SSD as extended memory.**

Intelligent layer streaming and caching for Mac â€” no need for 128GB RAM.

## The Problem

Large language models like LLaMA 70B require ~40GB+ RAM even with 4-bit quantization. Most MacBooks have 16â€“36GB unified memory. You either:
- Can't run the model at all
- Use llama.cpp's mmap, which thrashes your SSD with no intelligence
- Accept terrible performance from OS swap pressure

## The Solution

**ssd-llm** treats your fast Apple SSD as an intelligent extension of RAM:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SSD (2TB)  â”‚â”€â”€â”€â”€â–¶â”‚ Smart Cache  â”‚â”€â”€â”€â”€â–¶â”‚ Metal GPU â”‚
â”‚  Model File â”‚     â”‚ (Layer Pool) â”‚     â”‚ Inference  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–²                    â”‚
                     â”‚    Prefetch        â”‚ Compute
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Instead of loading the entire model, **ssd-llm** streams transformer layers on-demand from SSD to unified memory, computes them, and frees the memory. Predictive prefetching ensures the next layer is already loading while the current one is being computed.

## Key Features

- **ğŸ§± Layer-Level Streaming** â€” Only 1-2 transformer layers in RAM at once
- **ğŸ”® Predictive Prefetching** â€” Next layer loads asynchronously via `madvise(MADV_WILLNEED)` while GPU computes
- **ğŸ“¦ Smart LRU Cache** â€” Frequently used layers (embeddings, early attention) stay pinned in RAM
- **ğŸ—ºï¸ mmap + madvise** â€” OS-level memory-mapped files with intelligent page hints
- **âš¡ Metal Compute** â€” SIMD-optimized matmul, softmax, RoPE, RMSNorm with Metal shader foundation
- **ğŸ“„ GGUF Support** â€” Compatible with llama.cpp quantization formats (Q4_0, Q8_0, F16, F32)
- **ğŸ”¤ BPE Tokenizer** â€” Full Byte-Pair Encoding with SentencePiece support from GGUF vocabulary
- **ğŸ”Œ Ollama-compatible API** â€” Drop-in replacement server with OpenAI-compatible endpoint
- **ğŸ“¡ Streaming** â€” Real-time token-by-token streaming via chunked transfer (Ollama) and SSE (OpenAI)
- **ğŸ¯ Speculative Decoding** â€” Use a small draft model to propose tokens, verified by the target model for 2-3x speedup
- **ğŸ“¦ Batch Prefill** â€” Layer-major prompt processing: each layer loaded once for all prompt tokens, minimizing SSD reads
- **ğŸ›ï¸ Adaptive Draft Length** â€” Dynamically adjusts speculation depth K based on rolling acceptance rate
- **ğŸ“¦ Prompt Prefix Caching** â€” Reuse KV cache states for repeated prompt prefixes (system prompts, templates)
- **ğŸ”„ Continuous Batching** â€” Handle multiple concurrent requests, share layer loads across sequences
- **ğŸ”€ Tensor Parallelism** â€” Split matmul across multiple threads for better GPU/CPU utilization
- **ğŸªŸ Sliding Window Attention** â€” Limit attention to recent W tokens with optional sink tokens for bounded memory
- **ğŸ”— GQA Optimization** â€” Grouped-Query Attention with batched KV loads, auto-detected from model config
- **ğŸ’¾ Memory-Mapped KV Cache** â€” Spill KV cache to SSD via mmap when RAM is exhausted, enabling ultra-long contexts
- **âš¡ Flash Attention** â€” Memory-efficient fused attention kernel using online softmax (O(1) extra memory per head)
- **ğŸ“Š Structured Benchmark Suite** â€” JSON-exportable benchmarks with cold/warm/streaming scenarios for CI/CD
- **ğŸ¥ Health & Metrics API** â€” `/health` and `/metrics` endpoints with Prometheus-compatible output for production monitoring
- **ğŸ“¥ Model Downloader** â€” `ssd-llm pull` to download GGUF models from Hugging Face with resume support
- **âš™ï¸ Configuration File** â€” TOML config file support for persistent settings
- **ğŸ›‘ Graceful Shutdown** â€” Signal handling (SIGINT/SIGTERM) for clean server shutdown
- **ğŸ”§ CORS Support** â€” Full CORS preflight handling for browser-based clients
- **ğŸ“„ PagedAttention** â€” vLLM-style paged KV cache: fixed-size blocks allocated on-demand, copy-on-write for beam search/parallel sampling, near-zero memory waste, sequence forking
- **ğŸ§® Embeddings API** â€” OpenAI-compatible `/v1/embeddings` endpoint with L2-normalized vectors for RAG pipelines
- **ğŸ“‹ Models Listing** â€” OpenAI-compatible `/v1/models` endpoint for client discovery
- **ğŸ­ Chat Templates** â€” Auto-detected formatting for Llama 2, Llama 3, Mistral, Gemma, Phi-3, ChatML, and raw mode
- **ğŸ›‘ Stop Sequences** â€” Early generation termination on configurable stop strings
- **ğŸ” Repetition Penalties** â€” Repetition, frequency, and presence penalties to reduce repetitive output
- **ğŸ”¢ Complete K-Quant Family** â€” GPU-accelerated dequantization for all K-quants (Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, Q8_K) via Metal shaders, plus CPU fallback
- **ğŸ—œï¸ KV Cache Quantization** â€” INT8 per-row quantized KV cache for 4x memory reduction, enabling much longer context windows
- **ğŸ“ RoPE Scaling** â€” Linear, NTK-aware, and YaRN scaling methods for extended context windows beyond training length
- **ğŸ² Min-P Sampling** â€” Adaptive probability filtering that scales with model confidence for better quality/diversity trade-off
- **âœ‚ï¸ Tail-Free Sampling (TFS)** â€” Second-derivative based tail removal for cleaner distributions than top-p
- **ğŸ¯ Mirostat v1 & v2** â€” Adaptive perplexity-controlled sampling that maintains target surprise level for coherent text
- **ğŸ’¬ Interactive Chat** â€” `ssd-llm chat` for multi-turn conversations with history, undo, system prompts, and streaming output
- **ğŸ“‹ JSON Mode** â€” `response_format: { type: "json_object" }` for guaranteed valid JSON output via grammar-constrained generation
- **ğŸ”— LoRA Adapters** â€” Load LoRA adapters from GGUF files at inference time with configurable scaling, support for multiple simultaneous adapters
- **ğŸ› ï¸ Function Calling / Tool Use** â€” OpenAI-compatible `tools` parameter with function definitions, `tool_choice` control, parallel tool calls, argument validation, and multi-turn tool result messages
- **ğŸ§© Mixture of Experts (MoE)** â€” Sparse expert routing for models like Mixtral 8x7B/8x22B: top-K gating, SSD-friendly on-demand expert loading, batch expert pre-selection, Metal gating shader
- **ğŸ“ GBNF Grammar Constraints** â€” llama.cpp-compatible grammar-constrained generation for arbitrary structured output (SQL, XML, custom formats)
- **ğŸ‘ï¸ Vision/Multimodal** â€” CLIP ViT encoder for LLaVA-style image understanding, OpenAI-compatible image_url content, base64 and URL image input
- **ğŸ“ Criterion Benchmarks** â€” Reproducible micro-benchmarks for core operations (softmax, matvec, RoPE, RMSNorm)

## Quick Start

```bash
# Build
cargo build --release

# Download a model from Hugging Face
ssd-llm pull TheBloke/Llama-2-7B-GGUF:llama-2-7b.Q4_0.gguf

# List local models
ssd-llm models

# Show model info
ssd-llm info model.gguf

# Run inference with 8GB memory budget
ssd-llm run model.gguf --memory-budget 8G --prompt "Explain quantum computing"

# Benchmark SSD streaming performance
ssd-llm bench model.gguf --memory-budget 8G

# Start Ollama-compatible API server
ssd-llm serve model.gguf --memory-budget 8G --port 11434

# Speculative decoding with draft model (2-3x faster)
ssd-llm run model-70b.gguf --draft-model model-1b.gguf --prompt "Hello" --draft-ahead 5

# Adaptive draft length (auto-tunes K based on acceptance rate)
ssd-llm run model-70b.gguf --draft-model model-1b.gguf --prompt "Hello" --adaptive-draft

# Serve with speculative decoding
ssd-llm serve model-70b.gguf --draft-model model-1b.gguf --memory-budget 8G

# Enable prompt prefix caching (reuse KV states across requests)
ssd-llm run model.gguf --prompt "Hello" --prompt-cache

# Tensor parallelism (auto-detected or manual)
ssd-llm run model-70b.gguf --prompt "Hello" --tensor-parallel 4

# Continuous batching server (handles 8 concurrent requests)
ssd-llm serve model.gguf --memory-budget 8G --max-batch 8 --prompt-cache

# Sliding window attention (bounded memory for long contexts)
ssd-llm run model.gguf --prompt "Hello" --sliding-window 4096 --sink-tokens 4

# Memory-mapped KV cache (ultra-long contexts, spills to SSD)
ssd-llm run model.gguf --prompt "Hello" --mmap-kv --max-tokens 32768

# GQA is auto-detected â€” just run and see the optimization message
ssd-llm run llama-70b.gguf --prompt "Hello" --memory-budget 16G

# Generate default config file
ssd-llm config --init

# Run micro-benchmarks
cargo bench
```

## LoRA Adapters

Load fine-tuned LoRA adapters at inference time without modifying the base model:

```bash
# Run with a single LoRA adapter
ssd-llm run model.gguf --prompt "Hello" --lora adapter.gguf

# Multiple adapters with custom scaling
ssd-llm run model.gguf --prompt "Hello" --lora adapter1.gguf --lora adapter2.gguf --lora-scale 0.8

# Chat with LoRA adapter
ssd-llm chat model.gguf --lora adapter.gguf

# Serve with LoRA adapter
ssd-llm serve model.gguf --lora adapter.gguf
```

LoRA adapters are loaded from GGUF files containing `*.lora_a` / `*.lora_b` tensor pairs. The adapter weights are merged into the base model weights at layer-load time using the formula: `W' = W + (alpha/r) * scale * B @ A`. Rank and alpha are auto-detected from GGUF metadata.

### Grammar-Constrained Generation

Use GBNF grammars (llama.cpp-compatible) to constrain output to any structured format:

```bash
# Inline grammar
ssd-llm run model.gguf --prompt "Generate a color:" --grammar 'root ::= "red" | "green" | "blue"'

# Grammar from file
ssd-llm run model.gguf --prompt "Write SQL:" --grammar-file sql.gbnf

# Via API (Ollama endpoint)
curl -s http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [{"role": "user", "content": "List 3 colors as JSON"}],
  "grammar": "root ::= \"[\" ws item (\",\" ws item)* ws \"]\"\nitem ::= \"\\\"\" [a-z]+ \"\\\"\"\nws ::= [ ]*"
}'
```

GBNF grammars support: literals, character classes (`[a-z]`, `[^0-9]`), rule references, groups, quantifiers (`?`, `*`, `+`), and alternatives (`|`). The grammar engine filters token logits at each generation step, ensuring output always matches the defined grammar.

## API Server

The `serve` command starts an Ollama-compatible HTTP server:

```bash
ssd-llm serve model.gguf --memory-budget 8G
```

### Endpoints

| Endpoint | Method | Description |
|---|---|---|
| `/api/generate` | POST | Text generation (Ollama format) |
| `/api/chat` | POST | Chat completion (Ollama format) |
| `/api/tags` | GET | List loaded models |
| `/api/version` | GET | Server version |
| `/v1/chat/completions` | POST | OpenAI-compatible chat |
| `/health` | GET | Readiness probe (JSON) |
| `/metrics` | GET | Prometheus-compatible metrics |

### Usage with curl

```bash
# Ollama-style generation
curl -X POST http://localhost:11434/api/generate \
  -d '{"prompt": "What is Rust?", "num_predict": 128}'

# OpenAI-compatible chat
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello!"}], "max_tokens": 128}'
```

## How It Works

### Layer Streaming Architecture

Traditional LLM inference loads the entire model into RAM. **ssd-llm** takes a different approach:

1. **GGUF Parser** reads model metadata and tensor offsets without loading data
2. **mmap Loader** memory-maps the model file â€” the OS handles page faults
3. **Predictive Prefetcher** issues `madvise(MADV_WILLNEED)` for the next layer while the current one computes
4. **LRU Cache** keeps hot layers (embeddings, output weights) pinned in memory
5. **Eviction** calls `madvise(MADV_DONTNEED)` on completed layers to free page cache

### Why Apple Silicon?

Apple's Unified Memory Architecture is uniquely suited for this:

| Feature | Apple Silicon | Traditional PC |
|---|---|---|
| Memory | Unified (CPU+GPU shared) | Separate RAM + VRAM |
| SSD Speed | 5-7 GB/s (M3/M4 Pro) | 3-5 GB/s (NVMe) |
| Memory Bandwidth | 200-800 GB/s | 50-100 GB/s (DDR5) |
| GPU Access | Direct to unified memory | PCIe copy required |

The fast SSD + unified memory means layer streaming has very low overhead on Mac.

## Benchmarks

> v0.5 â€” Speculative decoding with draft model, KV cache rollback

| Model | Quant | Size | Memory Budget | Layer Load | Est. tok/s |
|---|---|---|---|---|---|
| LLaMA 7B | Q4_0 | 3.5 GB | 4 GB | ~2ms/layer | TBD |
| LLaMA 13B | Q4_0 | 7 GB | 8 GB | ~4ms/layer | TBD |
| LLaMA 70B | Q4_0 | 35 GB | 8 GB | ~8ms/layer | TBD |

Run `ssd-llm bench` on your machine to get actual numbers.

## Comparison

| Feature | ssd-llm | llama.cpp | Ollama |
|---|---|---|---|
| SSD Streaming | âœ… Intelligent | âš ï¸ Naive mmap | âŒ Full RAM |
| Predictive Prefetch | âœ… madvise hints | âŒ | âŒ |
| Memory Budget | âœ… Configurable | âŒ | âŒ |
| Layer-level Cache | âœ… LRU + pinning | âŒ | âŒ |
| Speculative Decoding | âœ… Draft model | âœ… (v0.6+) | âŒ |
| Continuous Batching | âœ… Layer-major | âœ… | âœ… |
| Prompt Caching | âœ… Prefix matching | âŒ | âŒ |
| Tensor Parallelism | âœ… Multi-thread | âœ… | âœ… (via llama.cpp) |
| Metal GPU | âœ… Shaders + SIMD | âœ… | âœ… (via llama.cpp) |
| GGUF Support | âœ… | âœ… | âœ… |
| Quantization | Q4_0, Q8_0, F16 | All | All |
| API Server | âœ… Ollama + OpenAI | âœ… | âœ… |

## Architecture

```
src/
  main.rs              â€” CLI + entry point
  model/
    gguf.rs            â€” GGUF v2/v3 parser
    loader.rs          â€” mmap-based lazy loader
    cache.rs           â€” LRU layer cache with memory budget
  inference/
    transformer.rs     â€” Layer-by-layer forward pass
    attention.rs       â€” Multi-Head Attention with KV cache (GQA support)
    kv_cache.rs        â€” Key-Value cache for autoregressive generation
    feed_forward.rs    â€” SwiGLU FFN
    sampler.rs         â€” Temperature, Top-K, Top-P sampling (xorshift64)
    speculative.rs     â€” Speculative decoding engine (draft + verify)
    tokenizer.rs       â€” BPE tokenizer with SentencePiece support
    prompt_cache.rs    â€” Prompt prefix KV state caching
    batch_scheduler.rs â€” Continuous batching scheduler
    tensor_parallel.rs â€” Multi-threaded tensor parallelism
    grammar.rs         â€” GBNF grammar parser + constrained generation engine
  metal/
    compute.rs         â€” Metal compute + SIMD-optimized ops (auto GPU dispatch)
    gpu.rs             â€” metal-rs GPU pipeline (real Metal compute)
    shaders/           â€” .metal compute shaders (matmul, rmsnorm, rope, softmax)
  ssd/
    streamer.rs        â€” SSD â†’ RAM streaming engine
    prefetch.rs        â€” Predictive prefetcher
    mmap_pool.rs       â€” mmap pool with madvise management
  api/
    server.rs          â€” Ollama-compatible HTTP API server (graceful shutdown, CORS)
    openai.rs          â€” OpenAI-compatible types + ChatML formatting
    metrics.rs         â€” Health & Prometheus metrics
  pull/
    mod.rs             â€” HuggingFace model downloader with resume support
  config.rs            â€” TOML configuration file support
  benchmark.rs         â€” Performance measurement
benches/
  inference_bench.rs   â€” Criterion micro-benchmarks (softmax, matvec, RoPE, SiLU)
```

## Speculative Decoding

Speculative decoding uses a small "draft" model (e.g. 1B parameters) to propose candidate tokens, then verifies them with the large target model. This is particularly effective for ssd-llm because:

1. **Draft model fits in RAM** â€” no SSD streaming needed for the small model
2. **Target model streams fewer times** â€” accepted draft tokens skip expensive SSD I/O
3. **Mathematically lossless** â€” the output distribution is identical to the target model

### How it works

```
Draft Model (1B, in RAM):    [tok1] â†’ [tok2] â†’ [tok3] â†’ [tok4] â†’ [tok5]
                                â†“        â†“        â†“        â†“        â†“
Target Model (70B, SSD):    verify   verify   verify   REJECT   resample
                                âœ“        âœ“        âœ“        âœ—        â†’tok4'
```

With a good draft model, 60-80% of tokens are accepted, meaning the target model does ~40% fewer forward passes. For SSD-streaming workloads this translates to 2-3x speedup.

### Configuration

- `--draft-model <path>` â€” Path to the draft GGUF model (same tokenizer family)
- `--draft-ahead <K>` â€” Number of tokens to draft per round (default: 5, try 3-8)

Higher `draft-ahead` values give more potential speedup but waste more compute on rejections. Start with 5 and tune based on your model pair's acceptance rate.

## Prior Art & Research

This project builds on insights from:

- **llama.cpp** â€” Uses mmap but with no intelligent page management
- **FlexGen** â€” SSD offloading for throughput-oriented inference
- **PowerInfer** â€” Sparsity-based selective loading
- **LLM in a Flash** (Apple Research) â€” Flash memory optimization for LLM inference
- **FlexInfer** â€” Flexible offloading with computation-I/O overlap

## Roadmap

- [x] v0.1 â€” GGUF parser, mmap loader, LRU cache, prefetcher, CPU inference
- [x] v0.2 â€” Metal compute foundation, SIMD ops, Ollama + OpenAI API server
- [x] v0.3 â€” KV cache, Metal shader compilation, SwiGLU FFN, quantized GPU kernels (Q4_0/Q8_0)
- [x] v0.4 â€” Full Metal GPU dispatch via metal-rs, BPE tokenizer, streaming responses
- [x] v0.5 â€” Speculative decoding with draft model, KV cache rollback
- [x] v0.6 â€” Batch prefill optimization, adaptive draft length
- [x] v0.7 â€” Continuous batching, prompt caching, tensor parallelism
- [x] v0.8 â€” Sliding window attention, GQA optimization, memory-mapped KV cache
- [x] v0.9 â€” Structured benchmark suite, flash attention, health/metrics API
- [x] v1.0 â€” Production-ready: model downloader, config files, graceful shutdown, criterion benchmarks, CORS, clippy-clean
- [x] v1.1 â€” OpenAI embeddings API (`/v1/embeddings`), models listing (`/v1/models`), L2-normalized embedding extraction
- [x] v1.2 â€” Chat templates (Llama 2/3, Mistral, Gemma, Phi-3, ChatML), stop sequences, repetition/frequency/presence penalties, proper token usage tracking
- [x] v1.3 â€” K-quant GPU dequantization (Q4_K, Q6_K Metal shaders), unified quantized matvec dispatch
- [x] v1.4 â€” INT8 KV cache quantization for 4x memory reduction
- [x] v1.5 â€” RoPE scaling (Linear, NTK-aware, YaRN) + Min-P sampling
- [x] v1.6 â€” Tail-Free Sampling (TFS) + Mirostat v1/v2 adaptive sampling
- [x] v1.7 â€” Interactive chat CLI + JSON mode for structured output
- [x] v1.8 â€” LoRA adapter support (load fine-tuned adapters from GGUF)
- [x] v1.9 â€” Function calling / Tool use (OpenAI-compatible, multi-turn, parallel calls)
- [x] v1.10 â€” Mixture of Experts (MoE) â€” sparse expert routing for Mixtral-style models
- [x] v1.11 â€” GBNF grammar-constrained generation for arbitrary structured output
- [x] v1.12 â€” Vision/Multimodal support (CLIP ViT encoder for LLaVA-style image understanding)
- [x] v1.13 â€” Ollama model management API (/api/show, /api/pull, /api/copy, /api/delete, /api/ps)
- [x] v1.14 â€” Q3_K + Q5_K GPU dequantization, Ollama /api/embed endpoint

## Requirements

- macOS 13+ (Apple Silicon recommended)
- Rust 1.75+
- GGUF model file (from [HuggingFace](https://huggingface.co/models?library=gguf))

## License

MIT
